{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c49f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387af88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"C:/Users/surendar.reddy/Documents/data Science projet_Data traine/DataTrained_Project_Week-1/bigdatamart_Train.csv\")\n",
    "df2=pd.read_csv(\"C:/Users/surendar.reddy/Documents/data Science projet_Data traine/DataTrained_Project_Week-1/bigdatamart_Test.csv\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "#decided to concat these data frames, As the imputed labels would be same in both the sets.\n",
    "df =df1.append(pd.DataFrame(data = df2),ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "#Lot of them are object type, Encoding needed\n",
    "#Item Identifyer might be just a unquie ID which would not be needed\n",
    "df2.info()\n",
    "df1.info()\n",
    "#As the Item_Weight cannot be imputed because \"mean imputing might not be relavent for all types of items types, Decided to impute based on the item type\n",
    "# Inspecting  Outlet_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_avg_weight = df.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "\n",
    "missing_values = df['Item_Weight'].isnull()\n",
    "print('Missing values: %d' %sum(missing_values))\n",
    "\n",
    "df.loc[missing_values,'Item_Weight']  = df.loc[missing_values,'Item_Identifier'].apply(lambda x: item_avg_weight.at[x,'Item_Weight'])\n",
    "print('Missing values after immputation %d' %sum(df['Item_Weight'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02548465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can We impute the missing value based on any other column\n",
    "\n",
    "df4=pd.DataFrame({})\n",
    "df4['Outlet_Type']=df['Outlet_Type']\n",
    "df4['Outlet_Size']=df['Outlet_Size']\n",
    "df4['Outlet_Type']=LE.fit_transform(df4['Outlet_Type'])\n",
    "df4.sort_values(['Outlet_Type'],inplace=True)\n",
    "df4=df4.groupby('Outlet_Type').apply(lambda x: x.ffill().bfill())\n",
    "#df4.drop_duplicates()\n",
    "#.ffill().sort_index()-This would help\n",
    "df5=pd.DataFrame(df4['Outlet_Size'])\n",
    "df['Outlet_Size']=df5['Outlet_Size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd37a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071aa70",
   "metadata": {},
   "source": [
    "#Will try to impute Outlet size_from Outlet type\n",
    "from scipy.stats import mode\n",
    "\n",
    "#Determing the mode for each\n",
    "outlet_size_mode = df.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x.astype('str')).mode[0]))\n",
    "print ('Mode for each Outlet_Type:')\n",
    "print (outlet_size_mode)\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "missing_values = df['Outlet_Size'].isnull() \n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm\n",
    "print ('\\nOrignal #missing: %d'% sum(missing_values))\n",
    "df.loc[missing_values,'Outlet_Size'] = df.loc[missing_values,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\n",
    "print (sum(df['Outlet_Size'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# Will Encode all the object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3933dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_features = df.select_dtypes(include=[np.object])\n",
    "categorial_features.head(2)\n",
    "\n",
    "#frequency of categories\n",
    "for col in categorial_features:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=[np.number])\n",
    "numerical_features.head(2)\n",
    "numerical_features.describe()\n",
    "#Looks okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54564ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross check both the columns\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "sns.catplot(x=\"Outlet_Type\", y=\"Outlet_Size\", jitter=False, data=df)\n",
    "plt.show\n",
    "#this plot show that Outlet Identify would add equal weightage so wold delete Outletsize from Data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c06843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e177647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Item_Fat_Content']=df['Item_Fat_Content'].replace({'Low Fat':0, 'LF':0, 'low fat':0,'Regular':1,'reg':1  })\n",
    "df['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Item_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "LE=preprocessing.LabelEncoder()\n",
    "\n",
    "df['Item_Identifier']=LE.fit_transform(df['Item_Identifier'])\n",
    "df['Item_Type']=LE.fit_transform(df['Item_Type'])\n",
    "df['Outlet_Identifier']=LE.fit_transform(df['Outlet_Identifier'])\n",
    "df['Outlet_Size']=LE.fit_transform(df['Outlet_Size'])\n",
    "df['Outlet_Location_Type']=LE.fit_transform(df['Outlet_Location_Type'])\n",
    "df['Outlet_Type']=LE.fit_transform(df['Outlet_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "#decided to delete the missing information then imputing the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b13f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the columns\n",
    "Corr=df.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(Corr,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820480f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20),facecolor='white')\n",
    "plotnumber=1\n",
    "\n",
    "for column in numerical_features:\n",
    "    if plotnumber<=12:\n",
    "        ax=plt.subplot(3,4,plotnumber)\n",
    "        sns.distplot(df[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.tight_layout()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b380d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features.skew()\n",
    "#Only Item visibility needs skewness correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c78ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Screwness Correction\n",
    "from sklearn.preprocessing import power_transform\n",
    "df6=pd.DataFrame(numerical_features['Item_Visibility'])\n",
    "df6=power_transform(df6)\n",
    "df_new=pd.DataFrame(df6,columns=['Item_Visibility'])\n",
    "df['Item_Visibility']=df_new['Item_Visibility']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the test train x and y\n",
    "Data_train=df[0: 8523]\n",
    "Data_train\n",
    "Data_test=df[8523:]\n",
    "Data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2\n",
    "#Seperated perfectly both the dataFrame after EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Multicollinearity\n",
    "x=Data_train.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "y=Data_train['Item_Outlet_Sales']\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = x.columns\n",
    "\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(x.values, i)\n",
    "                          for i in range(len(x.columns))]\n",
    "print(vif_data)\n",
    "#I will drop Outlet_Establishment_Year, Outlet_Location_Type with highest VIF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b28d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Data_train.drop(columns=['Item_Outlet_Sales','Outlet_Establishment_Year','Outlet_Location_Type'],axis=1)\n",
    "y=Data_train['Item_Outlet_Sales']\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = x.columns\n",
    "\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(x.values, i)\n",
    "                          for i in range(len(x.columns))]\n",
    "print(vif_data)\n",
    "#Looks good to proceed with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1104a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the fetures\n",
    "scaler=StandardScaler()\n",
    "x_scaled=scaler.fit_transform(x)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53835c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the data to the model\n",
    "x_train, x_test, y_train, y_test=train_test_split(x_scaled,y,test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ad7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg=LinearRegression()\n",
    "reg.fit(x_train,y_train)\n",
    "y_pred=reg.predict(x_test)\n",
    "reg_accuracy=metrics.r2_score(y_test,y_pred)\n",
    "print(reg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada=AdaBoostRegressor()\n",
    "ada.fit(x_train,y_train)\n",
    "y_pred=ada.predict(x_test)\n",
    "ada_accuracy=metrics.r2_score(y_test,y_pred)\n",
    "print(ada_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55707ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFR=RandomForestRegressor()\n",
    "RFR.fit(x_train,y_train,sample_weight=100)\n",
    "y_pred=RFR.predict(x_test)\n",
    "RFR_accuracy=metrics.r2_score(y_test,y_pred)\n",
    "print(RFR_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22650194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "k_f=KFold(n_splits=5, shuffle=True)\n",
    "k_f\n",
    "x_scaled=pd.DataFrame(x_scaled,columns=x.columns)\n",
    "from sklearn.metrics import accuracy_score\n",
    "k=5\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in k_f.split(x_scaled):\n",
    "    x_train , x_test = x_scaled.iloc[train_index,:],x_scaled.iloc[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    RFR.fit(x_train,y_train)\n",
    "    pred_values = RFR.predict(x_test)\n",
    "     \n",
    "    acc = metrics.r2_score(y_test,pred_values)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbdt_clf=GradientBoostingRegressor()\n",
    "gbdt_clf.fit(x_train,y_train)\n",
    "y_pred=gbdt_clf.predict(x_test)\n",
    "gbdt_clf_accuracy=metrics.r2_score(y_test,y_pred)\n",
    "gbdt_clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation got GBRX\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "k_f=KFold(n_splits=5, shuffle=True)\n",
    "k_f\n",
    "x_scaled=pd.DataFrame(x_scaled,columns=x.columns)\n",
    "from sklearn.metrics import accuracy_score\n",
    "k=5\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in k_f.split(x_scaled):\n",
    "    x_train , x_test = x_scaled.iloc[train_index,:],x_scaled.iloc[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    gbdt_clf.fit(x_train,y_train)\n",
    "    pred_values = gbdt_clf.predict(x_test)\n",
    "     \n",
    "    acc = metrics.r2_score(y_test,pred_values)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a521cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will Go with GradientbOOST rEGRESSOR AND hYPERTUNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbdt_clf=GradientBoostingRegressor()\n",
    "params={\"learning_rate\":[0.01, 0.001, 0.2],\n",
    "    \"n_estimators\":[15, 20]\n",
    "       }\n",
    "grd=GridSearchCV(gbdt_clf,param_grid=params,)\n",
    "grd.fit(x_train,y_train)\n",
    "gbdt_clf=grd.best_estimator_\n",
    "y_pred=gbdt_clf.predict(x_test)\n",
    "gbdt_clf_accuracy=metrics.r2_score(y_test,y_pred)\n",
    "gbdt_clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b17c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could not improve much on Hypertunning aftermultiple attems also.\n",
    "with open(\"gbdt_clf\", \"wb\") as f:\n",
    "    pickle.dump(gbdt_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "Data_test_scaled=scaler.fit_transform(Data_test)\n",
    "Data_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3564f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test_scaled=pd.DataFrame(Data_test_scaled)\n",
    "Data_test_scaled.drop(Data_test_scaled.columns[11], axis=1, inplace=True)\n",
    "#Removing Established year\n",
    "Data_test_scaled.drop(Data_test_scaled.columns[7], axis=1, inplace=True)\n",
    "#Removing Outlet location type\n",
    "Data_test_scaled.drop(Data_test_scaled.columns[9], axis=1, inplace=True)\n",
    "Data_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tESTING THE mODEL WITH THE tEST DATAsET\n",
    "\n",
    "y_pred_test=gbdt_clf.predict(Data_test_scaled)\n",
    "y_pred_test\n",
    "\n",
    "\n",
    "#gbdt_clf_accuracy=metrics.r2_score(y_test,y_pred_TEST)\n",
    "#gbdt_clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699663d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
